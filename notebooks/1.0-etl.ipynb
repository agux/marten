{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import multiprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlalchemy\n",
    "import exchange_calendars as xcals\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# import exchange_calendars as xcals\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# import pytz\n",
    "# import pandas as pd\n",
    "# from IPython.display import display, HTML\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.dialects.postgresql import insert, text\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from functools import lru_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.92\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()  # take environment variables from .env.\n",
    "\n",
    "module_path = os.getenv(\"LOCAL_AKSHARE_DEV_MODULE\")\n",
    "if module_path is not None and module_path not in sys.path:\n",
    "    sys.path.insert(0, module_path)\n",
    "import akshare as ak  # noqa: E402\n",
    "\n",
    "print(ak.__version__)\n",
    "\n",
    "DB_USER = os.getenv(\"DB_USER\")\n",
    "DB_PASSWORD = os.getenv(\"DB_PASSWORD\")\n",
    "DB_HOST = os.getenv(\"DB_HOST\")\n",
    "DB_PORT = os.getenv(\"DB_PORT\")\n",
    "DB_NAME = os.getenv(\"DB_NAME\")\n",
    "\n",
    "# Create an engine instance\n",
    "alchemyEngine = create_engine(\n",
    "    f\"postgresql+psycopg2://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}\",\n",
    "    pool_recycle=3600,\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "file_handler = logging.FileHandler(\"etl.log\")\n",
    "console_handler = logging.StreamHandler()\n",
    "\n",
    "# Step 4: Create a formatter\n",
    "formatter = logging.Formatter(\"%(name)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "# Step 5: Attach the formatter to the handlers\n",
    "file_handler.setFormatter(formatter)\n",
    "console_handler.setFormatter(formatter)\n",
    "\n",
    "# Step 6: Add the handlers to the logger\n",
    "logger.addHandler(file_handler)\n",
    "logger.addHandler(console_handler)\n",
    "\n",
    "xshg = xcals.get_calendar(\"XSHG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_on_conflict(table, conn, df, primary_keys):\n",
    "    \"\"\"\n",
    "    Insert new records, update existing records\n",
    "    \"\"\"\n",
    "    table = sqlalchemy.Table(table, sqlalchemy.MetaData(), autoload_with=conn)\n",
    "    insert_stmt = insert(table).values(df.to_dict(orient=\"records\"))\n",
    "    on_conflict_stmt = insert_stmt.on_conflict_do_update(\n",
    "        index_elements=primary_keys,\n",
    "        set_={c.key: c for c in insert_stmt.excluded if c.key not in primary_keys},\n",
    "    )\n",
    "    conn.execute(on_conflict_stmt)\n",
    "\n",
    "\n",
    "def ignore_on_conflict(table, conn, df, primary_keys):\n",
    "    \"\"\"\n",
    "    Insert new records, ignore existing records\n",
    "    \"\"\"\n",
    "    table = sqlalchemy.Table(table, sqlalchemy.MetaData(), autoload_with=conn)\n",
    "    insert_stmt = insert(table).values(df.to_dict(orient=\"records\"))\n",
    "    on_conflict_stmt = insert_stmt.on_conflict_do_nothing(index_elements=primary_keys)\n",
    "    conn.execute(on_conflict_stmt)\n",
    "\n",
    "\n",
    "def saveAsCsv(file_name_main: str, df):\n",
    "    \"\"\"\n",
    "    Save dataframe to CSV file\n",
    "    \"\"\"\n",
    "    # save to file\n",
    "    # Get the current timestamp to append to the filename\n",
    "    current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    # Save the dataframe to a csv file with timestamp as suffix. Need to properly encode and display Chinese characters.\n",
    "    df.to_csv(f\"{file_name_main}_{current_time}.csv\", encoding=\"utf_8_sig\", index=False)\n",
    "\n",
    "\n",
    "@lru_cache()\n",
    "def last_trade_date():\n",
    "    current_date = datetime.now().date()\n",
    "    # Iterate backwards from current_date until a valid session is found\n",
    "    last_session = current_date\n",
    "    while not xshg.is_session(last_session):\n",
    "        last_session -= timedelta(days=1)\n",
    "    return last_session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fund_etf_spot_em "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get laste fund / ETF data set for today (or latest trading date), and persists into database.\n",
    "\n",
    "df = ak.fund_etf_spot_em()\n",
    "df = df[\n",
    "    [\n",
    "        \"代码\",\n",
    "        \"名称\",\n",
    "        \"最新价\",\n",
    "        \"IOPV实时估值\",\n",
    "        \"基金折价率\",\n",
    "        \"涨跌额\",\n",
    "        \"涨跌幅\",\n",
    "        \"成交量\",\n",
    "        \"成交额\",\n",
    "        \"开盘价\",\n",
    "        \"最高价\",\n",
    "        \"最低价\",\n",
    "        \"昨收\",\n",
    "        \"换手率\",\n",
    "        \"量比\",\n",
    "        \"委比\",\n",
    "        \"外盘\",\n",
    "        \"内盘\",\n",
    "        \"主力净流入-净额\",\n",
    "        \"主力净流入-净占比\",\n",
    "        \"超大单净流入-净额\",\n",
    "        \"超大单净流入-净占比\",\n",
    "        \"大单净流入-净额\",\n",
    "        \"大单净流入-净占比\",\n",
    "        \"中单净流入-净额\",\n",
    "        \"中单净流入-净占比\",\n",
    "        \"小单净流入-净额\",\n",
    "        \"小单净流入-净占比\",\n",
    "        \"流通市值\",\n",
    "        \"总市值\",\n",
    "        \"最新份额\",\n",
    "        \"数据日期\",\n",
    "        \"更新时间\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "saveAsCsv(\"fund_etf_spot_em\", df)\n",
    "\n",
    "# Rename the columns of df to match the table's column names\n",
    "df = df.rename(\n",
    "    columns={\n",
    "        \"数据日期\": \"date\",\n",
    "        \"更新时间\": \"update_time\",\n",
    "        \"代码\": \"code\",\n",
    "        \"名称\": \"name\",\n",
    "        \"最新价\": \"latest_price\",\n",
    "        \"IOPV实时估值\": \"iopv\",\n",
    "        \"基金折价率\": \"fund_discount_rate\",\n",
    "        \"涨跌额\": \"change_amount\",\n",
    "        \"涨跌幅\": \"change_rate\",\n",
    "        \"成交量\": \"volume\",\n",
    "        \"成交额\": \"turnover\",\n",
    "        \"开盘价\": \"opening_price\",\n",
    "        \"最高价\": \"highest_price\",\n",
    "        \"最低价\": \"lowest_price\",\n",
    "        \"昨收\": \"previous_close\",\n",
    "        \"换手率\": \"turnover_rate\",\n",
    "        \"量比\": \"volume_ratio\",\n",
    "        \"委比\": \"order_ratio\",\n",
    "        \"外盘\": \"external_disc\",\n",
    "        \"内盘\": \"internal_disc\",\n",
    "        \"主力净流入-净额\": \"main_force_net_inflow_amount\",\n",
    "        \"主力净流入-净占比\": \"main_force_net_inflow_ratio\",\n",
    "        \"超大单净流入-净额\": \"super_large_net_inflow_amount\",\n",
    "        \"超大单净流入-净占比\": \"super_large_net_inflow_ratio\",\n",
    "        \"大单净流入-净额\": \"large_net_inflow_amount\",\n",
    "        \"大单净流入-净占比\": \"large_net_inflow_ratio\",\n",
    "        \"中单净流入-净额\": \"medium_net_inflow_amount\",\n",
    "        \"中单净流入-净占比\": \"medium_net_inflow_ratio\",\n",
    "        \"小单净流入-净额\": \"small_net_inflow_amount\",\n",
    "        \"小单净流入-净占比\": \"small_net_inflow_ratio\",\n",
    "        \"流通市值\": \"circulating_market_value\",\n",
    "        \"总市值\": \"total_market_value\",\n",
    "        \"最新份额\": \"latest_shares\",\n",
    "    }\n",
    ")\n",
    "\n",
    "with alchemyEngine.begin() as conn:\n",
    "    update_on_conflict(\"fund_etf_spot_em\", conn, df, [\"code\", \"date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fund_etf_perf_em"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fund_exchange_rank_em_df = ak.fund_exchange_rank_em()\n",
    "\n",
    "saveAsCsv(\"fund_exchange_rank_em\", fund_exchange_rank_em_df)\n",
    "\n",
    "column_mapping = {\n",
    "    \"序号\": \"id\",\n",
    "    \"基金代码\": \"fundcode\",\n",
    "    \"基金简称\": \"fundname\",\n",
    "    \"类型\": \"type\",\n",
    "    \"日期\": \"date\",\n",
    "    \"单位净值\": \"unitnav\",\n",
    "    \"累计净值\": \"accumulatednav\",\n",
    "    \"近1周\": \"pastweek\",\n",
    "    \"近1月\": \"pastmonth\",\n",
    "    \"近3月\": \"past3months\",\n",
    "    \"近6月\": \"past6months\",\n",
    "    \"近1年\": \"pastyear\",\n",
    "    \"近2年\": \"past2years\",\n",
    "    \"近3年\": \"past3years\",\n",
    "    \"今年来\": \"ytd\",\n",
    "    \"成立来\": \"sinceinception\",\n",
    "    \"成立日期\": \"inceptiondate\",\n",
    "}\n",
    "fund_exchange_rank_em_df.rename(columns=column_mapping, inplace=True)\n",
    "\n",
    "with alchemyEngine.begin() as conn:\n",
    "    update_on_conflict(\"fund_etf_perf_em\", conn, fund_exchange_rank_em_df, [\"fundcode\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get a full list of ETF fund"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve list from Sina\n",
    "fund_etf_category_sina_df = ak.fund_etf_category_sina(symbol=\"ETF基金\")\n",
    "\n",
    "# keep only 2 columns from `fund_etf_category_sina_df`: 代码, 名称.\n",
    "# split `代码` values by `exchange code` and `symbol` and store into 2 columns. No need to keep the `代码` column.\n",
    "# for example: 代码=sz159998, split into `exch=sz`, `symbol=159998`.\n",
    "df = fund_etf_category_sina_df[[\"代码\", \"名称\"]].copy()\n",
    "df.columns = [\"code\", \"name\"]\n",
    "df[[\"exch\", \"symbol\"]] = df[\"code\"].str.extract(r\"([a-z]+)(\\d+)\")\n",
    "df.drop(columns=[\"code\"], inplace=True)\n",
    "\n",
    "# Now, use the update_on_conflict function to insert or update the data\n",
    "with alchemyEngine.begin() as conn:\n",
    "    update_on_conflict(\"fund_etf_list_sina\", conn, df, [\"exch\", \"symbol\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Full List History Trade Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_date = datetime.now().strftime(\"%Y%m%d\")\n",
    "start_date = (datetime.now() - timedelta(days=20)).strftime(\"%Y%m%d\")\n",
    "# start_date = '19700101' # For entire history.\n",
    "\n",
    "\n",
    "# Function to fetch and process ETF data\n",
    "def fetch_and_process_etf(symbol):\n",
    "    try:\n",
    "        df = ak.fund_etf_hist_em(\n",
    "            symbol=symbol,\n",
    "            period=\"daily\",\n",
    "            start_date=\"19700101\",\n",
    "            end_date=end_date,\n",
    "            adjust=\"qfq\",\n",
    "        )\n",
    "        df[\"symbol\"] = symbol\n",
    "        df.columns = [\n",
    "            \"date\",\n",
    "            \"open\",\n",
    "            \"close\",\n",
    "            \"high\",\n",
    "            \"low\",\n",
    "            \"volume\",\n",
    "            \"turnover\",\n",
    "            \"amplitude\",\n",
    "            \"change_rate\",\n",
    "            \"change_amount\",\n",
    "            \"turnover_rate\",\n",
    "            \"symbol\",\n",
    "        ]\n",
    "        df = df[\n",
    "            [\n",
    "                \"symbol\",\n",
    "                \"date\",\n",
    "                \"open\",\n",
    "                \"close\",\n",
    "                \"high\",\n",
    "                \"low\",\n",
    "                \"volume\",\n",
    "                \"turnover\",\n",
    "                \"amplitude\",\n",
    "                \"change_rate\",\n",
    "                \"change_amount\",\n",
    "                \"turnover_rate\",\n",
    "            ]\n",
    "        ]\n",
    "        with alchemyEngine.begin() as conn:\n",
    "            ignore_on_conflict(\"fund_etf_daily_em\", conn, df, [\"symbol\", \"date\"])\n",
    "    except Exception:\n",
    "        logging.error(\n",
    "            f\"failed to get daily trade history data for {symbol}\", exc_info=True\n",
    "        )\n",
    "        return None\n",
    "    return df\n",
    "\n",
    "\n",
    "# Fetch the ETF list\n",
    "etf_list_df = pd.read_sql(\"SELECT symbol FROM fund_etf_list_sina\", alchemyEngine)\n",
    "\n",
    "# get the number of CPU cores\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "\n",
    "# Use ThreadPoolExecutor to fetch data in parallel\n",
    "with ThreadPoolExecutor(max_workers=num_cores) as executor:\n",
    "    futures = [\n",
    "        executor.submit(fetch_and_process_etf, symbol)\n",
    "        for symbol in etf_list_df[\"symbol\"]\n",
    "    ]\n",
    "    results = [future.result() for future in futures]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Performance Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get historical bond rate (risk-free interest rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \r"
     ]
    }
   ],
   "source": [
    "# start_date = (datetime.now() - timedelta(days=20)).strftime('%Y%m%d')\n",
    "start_date = None  # For entire history.\n",
    "\n",
    "bzur = ak.bond_zh_us_rate(start_date)\n",
    "bzur = bzur.rename(\n",
    "    columns={\n",
    "        \"日期\": \"date\",\n",
    "        \"中国国债收益率2年\": \"china_yield_2y\",\n",
    "        \"中国国债收益率5年\": \"china_yield_5y\",\n",
    "        \"中国国债收益率10年\": \"china_yield_10y\",\n",
    "        \"中国国债收益率30年\": \"china_yield_30y\",\n",
    "        \"中国国债收益率10年-2年\": \"china_yield_spread_10y_2y\",\n",
    "        \"中国GDP年增率\": \"china_gdp_growth\",\n",
    "        \"美国国债收益率2年\": \"us_yield_2y\",\n",
    "        \"美国国债收益率5年\": \"us_yield_5y\",\n",
    "        \"美国国债收益率10年\": \"us_yield_10y\",\n",
    "        \"美国国债收益率30年\": \"us_yield_30y\",\n",
    "        \"美国国债收益率10年-2年\": \"us_yield_spread_10y_2y\",\n",
    "        \"美国GDP年增率\": \"us_gdp_growth\",\n",
    "    }\n",
    ")\n",
    "with alchemyEngine.begin() as conn:\n",
    "    ignore_on_conflict(\"bond_metrics_em\", conn, bzur, [\"date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calc / Update metrics in fund_etf_perf_em table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           date  china_yield_2y\n",
      "0    2023-03-15          2.4329\n",
      "1    2023-03-16          2.4246\n",
      "2    2023-03-17          2.4134\n",
      "3    2023-03-20          2.3896\n",
      "4    2023-03-21          2.4025\n",
      "..          ...             ...\n",
      "265  2024-03-11          2.0754\n",
      "266  2024-03-12          2.1072\n",
      "267  2024-03-13          2.1025\n",
      "268  2024-03-14          2.1013\n",
      "269  2024-03-15          2.0810\n",
      "\n",
      "[270 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Testing:\n",
    "interval = 250  # assume 250 trading days annualy\n",
    "end_date = last_trade_date()\n",
    "start_date = (end_date - timedelta(days=interval)).strftime(\"%Y%m%d\")\n",
    "\n",
    "with alchemyEngine.begin() as conn:\n",
    "    start_date = (end_date - timedelta(days=366)).strftime(\"%Y%m%d\")\n",
    "    # select date, china_yield_2y from table `bond_metrics_em`, where date is between start_date and end_date (inclusive). Load into a dataframe.\n",
    "    query = \"\"\"SELECT date, china_yield_2y FROM bond_metrics_em WHERE date BETWEEN '{}' AND '{}'\"\"\".format(\n",
    "        start_date, end_date\n",
    "    )\n",
    "    bme_df = pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "interval = 250  # assume 250 trading days annualy\n",
    "end_date = last_trade_date()\n",
    "# start_date = (end_date - timedelta(days=interval)).strftime('%Y%m%d')\n",
    "# start_date = '19700101' # For entire history.\n",
    "\n",
    "# get 2-years CN bond IR as risk-free IR from bond_metrics_em table. 1-year series (natural dates).\n",
    "with alchemyEngine.begin() as conn:\n",
    "    start_date = (end_date - timedelta(days=366)).strftime(\"%Y%m%d\")\n",
    "    # select date, china_yield_2y from table `bond_metrics_em`, where date is between start_date and end_date (inclusive). Load into a dataframe.\n",
    "    query = \"\"\"SELECT date, china_yield_2y FROM bond_metrics_em WHERE date BETWEEN '{}' AND '{}'\"\"\".format(\n",
    "        start_date, end_date\n",
    "    )\n",
    "    bme_df = pd.read_sql(query, conn, parse_dates=[\"date\"])\n",
    "    # Convert annualized rate to a daily rate assuming continuous compounding\n",
    "    bme_df[\"china_yield_2y_daily\"] = np.exp(bme_df[\"china_yield_2y\"] / 365) - 1\n",
    "\n",
    "\n",
    "# load historical data from daily table and calc metrics, then update perf table\n",
    "def update_etf_metrics(symbol):\n",
    "    try:\n",
    "        with alchemyEngine.begin() as conn:\n",
    "            # load the latest (top) `interval` records of historical market data records from `fund_etf_daily_em` table for `symbol`, order by `date`.\n",
    "            # select columns: date, change_rate\n",
    "            query = \"\"\"SELECT date, change_rate FROM fund_etf_daily_em WHERE symbol = '{}' ORDER BY date DESC LIMIT {}\"\"\".format(\n",
    "                symbol, interval\n",
    "            )\n",
    "            df = pd.read_sql(query, conn, parse_dates=[\"date\"])\n",
    "            # merge df with bme_df by matching dates.\n",
    "            df = pd.merge_asof(\n",
    "                df.sort_values(\"date\"),\n",
    "                bme_df.sort_values(\"date\"),\n",
    "                on=\"date\",\n",
    "                direction=\"backward\",\n",
    "            ).dropna(subset=[\"change_rate\"])\n",
    "\n",
    "            # calculate the Sharpe ratio, Sortino ratio, and max drawdown with the time series data inside df.\n",
    "            df[\"excess_return\"] = df[\"change_rate\"] - df[\"china_yield_2y_daily\"]\n",
    "            # Annualize the excess return\n",
    "            annualized_excess_return = np.mean(df[\"excess_return\"])\n",
    "\n",
    "            # Calculate the standard deviation of the excess returns\n",
    "            std_dev = df[\"excess_return\"].std()\n",
    "\n",
    "            # Sharpe ratio\n",
    "            sharpe_ratio = annualized_excess_return / std_dev\n",
    "\n",
    "            # Calculate the downside deviation (Sortino ratio denominator)\n",
    "            downside_dev = df[df[\"excess_return\"] < 0][\"excess_return\"].std()\n",
    "\n",
    "            # Sortino ratio\n",
    "            sortino_ratio = (\n",
    "                annualized_excess_return / downside_dev if downside_dev > 0 else np.nan\n",
    "            )\n",
    "\n",
    "            # To calculate max drawdown, get the cummulative_returns\n",
    "            df[\"cumulative_returns\"] = np.cumprod(1 + df[\"change_rate\"]) - 1\n",
    "            # Calculate the maximum cumulative return up to each point\n",
    "            peak = np.maximum.accumulate(df[\"cumulative_returns\"])\n",
    "            # Calculate drawdown as the difference between the current value and the peak\n",
    "            drawdown = (df[\"cumulative_returns\"] - peak) / peak\n",
    "            # Calculate max drawdown\n",
    "            max_drawdown = np.min(drawdown)  # This is a negative number\n",
    "\n",
    "            # update the `sharperatio, sortinoratio, maxdrawdown` columns for `symbol` in the table `fund_etf_perf_em` using the calculated metrics.\n",
    "            update_query = text(\n",
    "                \"UPDATE fund_etf_perf_em SET sharperatio = :sharperatio, sortinoratio = :sortinoratio, maxdrawdown = :maxdrawdown WHERE fundcode = :fundcode\"\n",
    "            )\n",
    "            params = {\n",
    "                \"sharperatio\": round(sharpe_ratio, 2)\n",
    "                if not np.isinf(sharpe_ratio)\n",
    "                else np.nan,\n",
    "                \"sortinoratio\": round(sortino_ratio, 2)\n",
    "                if not np.isinf(sortino_ratio)\n",
    "                else np.nan,\n",
    "                \"maxdrawdown\": round(max_drawdown, 2)\n",
    "                if not np.isinf(max_drawdown)\n",
    "                else np.nan,\n",
    "                \"fundcode\": symbol,\n",
    "            }\n",
    "            conn.execute(update_query, params)\n",
    "\n",
    "    except Exception:\n",
    "        logging.error(f\"failed to update ETF metrics for {symbol}\", exc_info=True)\n",
    "        return None\n",
    "    return df\n",
    "\n",
    "\n",
    "# Fetch the ETF list\n",
    "etf_list_df = pd.read_sql(\"SELECT symbol FROM fund_etf_list_sina\", alchemyEngine)\n",
    "\n",
    "# get the number of CPU cores\n",
    "num_proc = int((multiprocessing.cpu_count() + 1) / 2.0)\n",
    "\n",
    "# Use ThreadPoolExecutor to fetch data in parallel\n",
    "with ThreadPoolExecutor(max_workers=num_proc) as executor:\n",
    "    futures = [\n",
    "        executor.submit(update_etf_metrics, symbol) for symbol in etf_list_df[\"symbol\"]\n",
    "    ]\n",
    "    results = [future.result() for future in futures]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_3.12.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
