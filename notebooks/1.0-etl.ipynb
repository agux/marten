{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.92\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import multiprocessing\n",
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "from dotenv import load_dotenv\n",
    "# import exchange_calendars as xcals\n",
    "from datetime import datetime, timedelta\n",
    "# import pytz\n",
    "# import pandas as pd\n",
    "# from IPython.display import display, HTML\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.dialects.postgresql import insert\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "module_path = \"/Users/jx/ProgramData/python/akshare\"\n",
    "if module_path not in sys.path:\n",
    "    sys.path.insert(0, module_path)\n",
    "\n",
    "import akshare as ak  # noqa: E402\n",
    "\n",
    "print(ak.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()  # take environment variables from .env.\n",
    "\n",
    "DB_USER = os.getenv('DB_USER')\n",
    "DB_PASSWORD = os.getenv('DB_PASSWORD')\n",
    "DB_HOST = os.getenv('DB_HOST')\n",
    "DB_PORT = os.getenv('DB_PORT')\n",
    "DB_NAME = os.getenv('DB_NAME')\n",
    "\n",
    "# Create an engine instance\n",
    "alchemyEngine = create_engine(\n",
    "    f\"postgresql+psycopg2://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}\",\n",
    "    pool_recycle=3600,\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "file_handler = logging.FileHandler('etl.log')\n",
    "console_handler = logging.StreamHandler()\n",
    "\n",
    "# Step 4: Create a formatter\n",
    "formatter = logging.Formatter('%(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Step 5: Attach the formatter to the handlers\n",
    "file_handler.setFormatter(formatter)\n",
    "console_handler.setFormatter(formatter)\n",
    "\n",
    "# Step 6: Add the handlers to the logger\n",
    "logger.addHandler(file_handler)\n",
    "logger.addHandler(console_handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_on_conflict(table, conn, df, primary_keys):\n",
    "    \"\"\"\n",
    "    Insert new records, update existing records\n",
    "    \"\"\"\n",
    "    table = sqlalchemy.Table(table, sqlalchemy.MetaData(), autoload_with=conn)\n",
    "    insert_stmt = insert(table).values(df.to_dict(orient='records'))\n",
    "    on_conflict_stmt = insert_stmt.on_conflict_do_update(\n",
    "        index_elements=primary_keys,\n",
    "        set_={c.key: c for c in insert_stmt.excluded if c.key not in primary_keys}\n",
    "    )\n",
    "    conn.execute(on_conflict_stmt)\n",
    "\n",
    "def ignore_on_conflict(table, conn, df, primary_keys):\n",
    "    \"\"\"\n",
    "    Insert new records, ignore existing records\n",
    "    \"\"\"\n",
    "    table = sqlalchemy.Table(table, sqlalchemy.MetaData(), autoload_with=conn)\n",
    "    insert_stmt = insert(table).values(df.to_dict(orient='records'))\n",
    "    on_conflict_stmt = insert_stmt.on_conflict_do_nothing(\n",
    "        index_elements=primary_keys\n",
    "    )\n",
    "    conn.execute(on_conflict_stmt)\n",
    "\n",
    "def saveAsCsv(file_name_main: str, df):\n",
    "    \"\"\"\n",
    "    Save dataframe to CSV file\n",
    "    \"\"\"\n",
    "    # save to file\n",
    "    # Get the current timestamp to append to the filename\n",
    "    current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    # Save the dataframe to a csv file with timestamp as suffix. Need to properly encode and display Chinese characters.\n",
    "    df.to_csv(\n",
    "        f\"{file_name_main}_{current_time}.csv\", encoding=\"utf_8_sig\", index=False\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fund_etf_spot_em "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get laste fund / ETF data set for today (or latest trading date), and persists into database.\n",
    "\n",
    "df = ak.fund_etf_spot_em()\n",
    "df = df[\n",
    "    [\n",
    "        \"代码\",\n",
    "        \"名称\",\n",
    "        \"最新价\",\n",
    "        \"IOPV实时估值\",\n",
    "        \"基金折价率\",\n",
    "        \"涨跌额\",\n",
    "        \"涨跌幅\",\n",
    "        \"成交量\",\n",
    "        \"成交额\",\n",
    "        \"开盘价\",\n",
    "        \"最高价\",\n",
    "        \"最低价\",\n",
    "        \"昨收\",\n",
    "        \"换手率\",\n",
    "        \"量比\",\n",
    "        \"委比\",\n",
    "        \"外盘\",\n",
    "        \"内盘\",\n",
    "        \"主力净流入-净额\",\n",
    "        \"主力净流入-净占比\",\n",
    "        \"超大单净流入-净额\",\n",
    "        \"超大单净流入-净占比\",\n",
    "        \"大单净流入-净额\",\n",
    "        \"大单净流入-净占比\",\n",
    "        \"中单净流入-净额\",\n",
    "        \"中单净流入-净占比\",\n",
    "        \"小单净流入-净额\",\n",
    "        \"小单净流入-净占比\",\n",
    "        \"流通市值\",\n",
    "        \"总市值\",\n",
    "        \"最新份额\",\n",
    "        \"数据日期\",\n",
    "        \"更新时间\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "saveAsCsv(\"fund_etf_spot_em\", df)\n",
    "\n",
    "# Rename the columns of df to match the table's column names\n",
    "df = df.rename(\n",
    "    columns={\n",
    "        \"数据日期\": \"date\",\n",
    "        \"更新时间\": \"update_time\",\n",
    "        \"代码\": \"code\",\n",
    "        \"名称\": \"name\",\n",
    "        \"最新价\": \"latest_price\",\n",
    "        \"IOPV实时估值\": \"iopv\",\n",
    "        \"基金折价率\": \"fund_discount_rate\",\n",
    "        \"涨跌额\": \"change_amount\",\n",
    "        \"涨跌幅\": \"change_rate\",\n",
    "        \"成交量\": \"volume\",\n",
    "        \"成交额\": \"turnover\",\n",
    "        \"开盘价\": \"opening_price\",\n",
    "        \"最高价\": \"highest_price\",\n",
    "        \"最低价\": \"lowest_price\",\n",
    "        \"昨收\": \"previous_close\",\n",
    "        \"换手率\": \"turnover_rate\",\n",
    "        \"量比\": \"volume_ratio\",\n",
    "        \"委比\": \"order_ratio\",\n",
    "        \"外盘\": \"external_disc\",\n",
    "        \"内盘\": \"internal_disc\",\n",
    "        \"主力净流入-净额\": \"main_force_net_inflow_amount\",\n",
    "        \"主力净流入-净占比\": \"main_force_net_inflow_ratio\",\n",
    "        \"超大单净流入-净额\": \"super_large_net_inflow_amount\",\n",
    "        \"超大单净流入-净占比\": \"super_large_net_inflow_ratio\",\n",
    "        \"大单净流入-净额\": \"large_net_inflow_amount\",\n",
    "        \"大单净流入-净占比\": \"large_net_inflow_ratio\",\n",
    "        \"中单净流入-净额\": \"medium_net_inflow_amount\",\n",
    "        \"中单净流入-净占比\": \"medium_net_inflow_ratio\",\n",
    "        \"小单净流入-净额\": \"small_net_inflow_amount\",\n",
    "        \"小单净流入-净占比\": \"small_net_inflow_ratio\",\n",
    "        \"流通市值\": \"circulating_market_value\",\n",
    "        \"总市值\": \"total_market_value\",\n",
    "        \"最新份额\": \"latest_shares\",\n",
    "    }\n",
    ")\n",
    "\n",
    "with alchemyEngine.begin() as conn:\n",
    "    update_on_conflict('fund_etf_spot_em', conn, df, ['code', 'date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fund_etf_perf_em"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fund_exchange_rank_em_df = ak.fund_exchange_rank_em()\n",
    "\n",
    "saveAsCsv(\"fund_exchange_rank_em\", fund_exchange_rank_em_df)\n",
    "\n",
    "column_mapping = {\n",
    "    '序号': 'id',\n",
    "    '基金代码': 'fundcode',\n",
    "    '基金简称': 'fundname',\n",
    "    '类型': 'type',\n",
    "    '日期': 'date',\n",
    "    '单位净值': 'unitnav',\n",
    "    '累计净值': 'accumulatednav',\n",
    "    '近1周': 'pastweek',\n",
    "    '近1月': 'pastmonth',\n",
    "    '近3月': 'past3months',\n",
    "    '近6月': 'past6months',\n",
    "    '近1年': 'pastyear',\n",
    "    '近2年': 'past2years',\n",
    "    '近3年': 'past3years',\n",
    "    '今年来': 'ytd',\n",
    "    '成立来': 'sinceinception',\n",
    "    '成立日期': 'inceptiondate'\n",
    "}\n",
    "fund_exchange_rank_em_df.rename(columns=column_mapping, inplace=True)\n",
    "\n",
    "with alchemyEngine.begin() as conn:\n",
    "    update_on_conflict('fund_etf_perf_em', conn, fund_exchange_rank_em_df, ['fundcode'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get a full list of ETF fund"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve list from Sina\n",
    "fund_etf_category_sina_df = ak.fund_etf_category_sina(symbol=\"ETF基金\")\n",
    "\n",
    "# keep only 2 columns from `fund_etf_category_sina_df`: 代码, 名称.\n",
    "# split `代码` values by `exchange code` and `symbol` and store into 2 columns. No need to keep the `代码` column.\n",
    "# for example: 代码=sz159998, split into `exch=sz`, `symbol=159998`.\n",
    "df = fund_etf_category_sina_df[['代码', '名称']].copy()\n",
    "df.columns = ['code', 'name']\n",
    "df[['exch', 'symbol']] = df['code'].str.extract(r'([a-z]+)(\\d+)')\n",
    "df.drop(columns=['code'], inplace=True)\n",
    "\n",
    "# Now, use the update_on_conflict function to insert or update the data\n",
    "with alchemyEngine.begin() as conn:\n",
    "    update_on_conflict('fund_etf_list_sina', conn, df, ['exch', 'symbol'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Full List History Trade Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_date = datetime.now().strftime('%Y%m%d')\n",
    "start_date = (datetime.now() - timedelta(days=20)).strftime('%Y%m%d')\n",
    "# start_date = '19700101' # For entire history.\n",
    "\n",
    "# Function to fetch and process ETF data\n",
    "def fetch_and_process_etf(symbol):\n",
    "    try:\n",
    "        df = ak.fund_etf_hist_em(symbol=symbol, period='daily', start_date='19700101', end_date=end_date, adjust='qfq')\n",
    "        df['symbol'] = symbol\n",
    "        df.columns = ['date', 'open', 'close', 'high', 'low', 'volume', 'turnover', 'amplitude', 'change_rate', 'change_amount', 'turnover_rate', 'symbol']\n",
    "        df = df[['symbol', 'date', 'open', 'close', 'high', 'low', 'volume', 'turnover', 'amplitude', 'change_rate', 'change_amount', 'turnover_rate']]\n",
    "        with alchemyEngine.begin() as conn:\n",
    "            ignore_on_conflict('fund_etf_daily_em', conn, df, ['symbol', 'date'])\n",
    "    except Exception:\n",
    "        logging.error(f'failed to get daily trade history data for {symbol}', exc_info=True)\n",
    "        return None\n",
    "    return df\n",
    "\n",
    "# Fetch the ETF list\n",
    "etf_list_df = pd.read_sql('SELECT symbol FROM fund_etf_list_sina', alchemyEngine)\n",
    "\n",
    "# get the number of CPU cores\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "\n",
    "# Use ThreadPoolExecutor to fetch data in parallel\n",
    "with ThreadPoolExecutor(max_workers=num_cores) as executor:\n",
    "    futures = [executor.submit(fetch_and_process_etf, symbol) for symbol in etf_list_df['symbol']]\n",
    "    results = [future.result() for future in futures]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_3.12.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
