{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "from neuralforecast import NeuralForecast # type: ignore\n",
    "from neuralforecast.auto import AutoTimeMixer  # type: ignore\n",
    "from neuralforecast.models import TimeMixer, SOFTS  # type: ignore\n",
    "from neuralforecast.losses.pytorch import HuberLoss  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()  # take environment variables from .env.\n",
    "\n",
    "DB_USER = os.getenv(\"DB_USER\")\n",
    "DB_PASSWORD = os.getenv(\"DB_PASSWORD\")\n",
    "DB_HOST = os.getenv(\"DB_HOST\")\n",
    "DB_PORT = os.getenv(\"DB_PORT\")\n",
    "DB_NAME = os.getenv(\"DB_NAME\")\n",
    "db_url = f\"postgresql+psycopg2://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
    "\n",
    "alchemyEngine = create_engine(\n",
    "    db_url,\n",
    "    pool_recycle=3600,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"\"\"\n",
    "# with cte as (\n",
    "# SELECT \"date\", \"open\", \"close\", high, low, volume, amount, open_preclose_rate, high_preclose_rate, low_preclose_rate, vol_change_rate, amt_change_rate, change_rate\n",
    "# FROM index_daily_em_view\n",
    "# where symbol = '399673'\n",
    "# order by date desc\n",
    "# limit 1200\n",
    "# ) select * from cte order by date\n",
    "# \"\"\"\n",
    "query = \"\"\"\n",
    "with cte as (\n",
    "SELECT \"date\", \"open\", \"close\", high, low, volume, amount, open_preclose_rate, high_preclose_rate, low_preclose_rate, vol_change_rate, amt_change_rate, change_rate\n",
    "FROM index_daily_em_view \n",
    "where symbol = '399673'\n",
    "and change_rate is not null\n",
    ") select * from cte order by date\n",
    "\"\"\"\n",
    "\n",
    "raw_df = pd.read_sql(query, alchemyEngine, parse_dates=[\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = raw_df.rename(columns={\"date\":\"ds\", \"change_rate\":\"y\"})\n",
    "df.insert(0, \"unique_id\", \"399673\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unique_id                     object\n",
       "ds                    datetime64[ns]\n",
       "open                         float64\n",
       "close                        float64\n",
       "high                         float64\n",
       "low                          float64\n",
       "volume                       float64\n",
       "amount                       float64\n",
       "open_preclose_rate           float64\n",
       "high_preclose_rate           float64\n",
       "low_preclose_rate            float64\n",
       "vol_change_rate              float64\n",
       "amt_change_rate              float64\n",
       "y                            float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2488"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optional df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "      <th>open_preclose_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2481</th>\n",
       "      <td>399673</td>\n",
       "      <td>2024-08-27</td>\n",
       "      <td>-0.87590</td>\n",
       "      <td>-0.56837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2482</th>\n",
       "      <td>399673</td>\n",
       "      <td>2024-08-28</td>\n",
       "      <td>-0.06461</td>\n",
       "      <td>-0.27391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2483</th>\n",
       "      <td>399673</td>\n",
       "      <td>2024-08-29</td>\n",
       "      <td>0.50911</td>\n",
       "      <td>-1.45864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2484</th>\n",
       "      <td>399673</td>\n",
       "      <td>2024-08-30</td>\n",
       "      <td>2.85494</td>\n",
       "      <td>0.22245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2485</th>\n",
       "      <td>399673</td>\n",
       "      <td>2024-09-02</td>\n",
       "      <td>-2.91054</td>\n",
       "      <td>-0.26187</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     unique_id         ds        y  open_preclose_rate\n",
       "2481    399673 2024-08-27 -0.87590            -0.56837\n",
       "2482    399673 2024-08-28 -0.06461            -0.27391\n",
       "2483    399673 2024-08-29  0.50911            -1.45864\n",
       "2484    399673 2024-08-30  2.85494             0.22245\n",
       "2485    399673 2024-09-02 -2.91054            -0.26187"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# handle univariate first, to avoid the following error\n",
    "# 'DataFrame' object has no attribute 'temporal_cols'\n",
    "df_train = df[[\"unique_id\", \"ds\", \"y\", \"open_preclose_rate\"]]\n",
    "df_train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle univariate first, to avoid the following error\n",
    "# 'DataFrame' object has no attribute 'temporal_cols'\n",
    "df_train = df[[\"unique_id\", \"ds\", \"y\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_loss': tensor(1.3861),\n",
       " 'train_loss_step': tensor(1.3861),\n",
       " 'train_loss_epoch': tensor(1.3861),\n",
       " 'valid_loss': tensor(1.3116),\n",
       " 'ptl/val_loss': tensor(1.3116)}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nf.models[0].metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimeMixer"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nf.models[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = nf.models[0].metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.386099934577942"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(metrics[\"train_loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3116085529327393"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(metrics[\"valid_loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE()  MAE()\n"
     ]
    }
   ],
   "source": [
    "print(f\"{nf.models[0].loss}  {nf.models[0].valid_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHECKPOINT_HYPER_PARAMS_KEY\n",
      "CHECKPOINT_HYPER_PARAMS_NAME\n",
      "CHECKPOINT_HYPER_PARAMS_TYPE\n",
      "EXOGENOUS_FUTR\n",
      "EXOGENOUS_HIST\n",
      "EXOGENOUS_STAT\n",
      "SAMPLING_TYPE\n",
      "T_destination\n",
      "_LightningModule__check_allowed\n",
      "_LightningModule__check_not_nested\n",
      "_LightningModule__to_tensor\n",
      "_TimeMixer__multi_scale_process_inputs\n",
      "__annotations__\n",
      "__call__\n",
      "__class__\n",
      "__delattr__\n",
      "__dict__\n",
      "__dir__\n",
      "__doc__\n",
      "__eq__\n",
      "__format__\n",
      "__ge__\n",
      "__getattr__\n",
      "__getattribute__\n",
      "__getstate__\n",
      "__gt__\n",
      "__hash__\n",
      "__init__\n",
      "__init_subclass__\n",
      "__jit_unused_properties__\n",
      "__le__\n",
      "__lt__\n",
      "__module__\n",
      "__ne__\n",
      "__new__\n",
      "__reduce__\n",
      "__reduce_ex__\n",
      "__repr__\n",
      "__setattr__\n",
      "__setstate__\n",
      "__sizeof__\n",
      "__str__\n",
      "__subclasshook__\n",
      "__weakref__\n",
      "_apply\n",
      "_apply_batch_transfer_handler\n",
      "_automatic_optimization\n",
      "_backward_hooks\n",
      "_backward_pre_hooks\n",
      "_buffers\n",
      "_call_batch_hook\n",
      "_call_impl\n",
      "_check_exog\n",
      "_compiled_call_impl\n",
      "_compiler_ctx\n",
      "_create_windows\n",
      "_current_fx_name\n",
      "_device\n",
      "_device_mesh\n",
      "_dtype\n",
      "_example_input_array\n",
      "_fabric\n",
      "_fabric_optimizers\n",
      "_fit\n",
      "_fit_distributed\n",
      "_forward_hooks\n",
      "_forward_hooks_always_called\n",
      "_forward_hooks_with_kwargs\n",
      "_forward_pre_hooks\n",
      "_forward_pre_hooks_with_kwargs\n",
      "_get_backward_hooks\n",
      "_get_backward_pre_hooks\n",
      "_get_name\n",
      "_get_temporal_exogenous_cols\n",
      "_hparams\n",
      "_hparams_initial\n",
      "_hparams_name\n",
      "_inv_normalization\n",
      "_is_full_backward_hook\n",
      "_jit_is_scripting\n",
      "_load_from_state_dict\n",
      "_load_state_dict_post_hooks\n",
      "_load_state_dict_pre_hooks\n",
      "_log_dict_through_fabric\n",
      "_log_hyperparams\n",
      "_maybe_warn_non_full_backward_hook\n",
      "_metric_attributes\n",
      "_modules\n",
      "_named_members\n",
      "_non_persistent_buffers_set\n",
      "_normalization\n",
      "_on_before_batch_transfer\n",
      "_param_requires_grad_state\n",
      "_parameters\n",
      "_parse_windows\n",
      "_register_load_state_dict_pre_hook\n",
      "_register_sharded_tensor_state_dict_hooks_if_available\n",
      "_register_state_dict_hook\n",
      "_replicate_for_data_parallel\n",
      "_restart_seed\n",
      "_save_to_state_dict\n",
      "_set_hparams\n",
      "_set_quantile_for_iqloss\n",
      "_slow_forward\n",
      "_state_dict_hooks\n",
      "_state_dict_pre_hooks\n",
      "_strict_loading\n",
      "_to_hparams_dict\n",
      "_verify_is_manual_optimization\n",
      "_version\n",
      "_wrapped_call_impl\n",
      "add_module\n",
      "alias\n",
      "all_gather\n",
      "allow_zero_length_dataloader_with_multiple_devices\n",
      "apply\n",
      "automatic_optimization\n",
      "backward\n",
      "batch_size\n",
      "bfloat16\n",
      "buffers\n",
      "c_out\n",
      "call_super_init\n",
      "channel_independence\n",
      "children\n",
      "clip_gradients\n",
      "compile\n",
      "configure_callbacks\n",
      "configure_gradient_clipping\n",
      "configure_model\n",
      "configure_optimizers\n",
      "configure_sharded_model\n",
      "cpu\n",
      "cuda\n",
      "current_epoch\n",
      "d_ff\n",
      "d_model\n",
      "decomp_method\n",
      "decompose\n",
      "decompose_forecast\n",
      "device\n",
      "device_mesh\n",
      "double\n",
      "down_sampling_layers\n",
      "down_sampling_method\n",
      "down_sampling_window\n",
      "drop_last_loader\n",
      "dropout\n",
      "dtype\n",
      "dump_patches\n",
      "e_layers\n",
      "early_stop_patience_steps\n",
      "enc_embedding\n",
      "enc_in\n",
      "eval\n",
      "example_input_array\n",
      "extra_repr\n",
      "fabric\n",
      "fit\n",
      "float\n",
      "forecast\n",
      "forward\n",
      "freeze\n",
      "futr_exog_list\n",
      "futr_exog_size\n",
      "future_multi_mixing\n",
      "get_buffer\n",
      "get_extra_state\n",
      "get_parameter\n",
      "get_submodule\n",
      "get_test_size\n",
      "global_rank\n",
      "global_step\n",
      "h\n",
      "half\n",
      "hist_exog_list\n",
      "hist_exog_size\n",
      "hparams\n",
      "hparams_initial\n",
      "input_size\n",
      "ipu\n",
      "label_len\n",
      "learning_rate\n",
      "load\n",
      "load_from_checkpoint\n",
      "load_state_dict\n",
      "local_rank\n",
      "log\n",
      "log_dict\n",
      "logger\n",
      "loggers\n",
      "loss\n",
      "lr_decay_steps\n",
      "lr_scheduler\n",
      "lr_scheduler_kwargs\n",
      "lr_scheduler_step\n",
      "lr_schedulers\n",
      "manual_backward\n",
      "max_steps\n",
      "metrics\n",
      "modules\n",
      "moving_avg\n",
      "n_series\n",
      "named_buffers\n",
      "named_children\n",
      "named_modules\n",
      "named_parameters\n",
      "normalize_layers\n",
      "num_lr_decays\n",
      "num_workers_loader\n",
      "on_after_backward\n",
      "on_after_batch_transfer\n",
      "on_before_backward\n",
      "on_before_batch_transfer\n",
      "on_before_optimizer_step\n",
      "on_before_zero_grad\n",
      "on_fit_end\n",
      "on_fit_start\n",
      "on_gpu\n",
      "on_load_checkpoint\n",
      "on_predict_batch_end\n",
      "on_predict_batch_start\n",
      "on_predict_end\n",
      "on_predict_epoch_end\n",
      "on_predict_epoch_start\n",
      "on_predict_model_eval\n",
      "on_predict_start\n",
      "on_save_checkpoint\n",
      "on_test_batch_end\n",
      "on_test_batch_start\n",
      "on_test_end\n",
      "on_test_epoch_end\n",
      "on_test_epoch_start\n",
      "on_test_model_eval\n",
      "on_test_model_train\n",
      "on_test_start\n",
      "on_train_batch_end\n",
      "on_train_batch_start\n",
      "on_train_end\n",
      "on_train_epoch_end\n",
      "on_train_epoch_start\n",
      "on_train_start\n",
      "on_validation_batch_end\n",
      "on_validation_batch_start\n",
      "on_validation_end\n",
      "on_validation_epoch_end\n",
      "on_validation_epoch_start\n",
      "on_validation_model_eval\n",
      "on_validation_model_train\n",
      "on_validation_model_zero_grad\n",
      "on_validation_start\n",
      "optimizer\n",
      "optimizer_kwargs\n",
      "optimizer_step\n",
      "optimizer_zero_grad\n",
      "optimizers\n",
      "out_projection\n",
      "out_res_layers\n",
      "padder\n",
      "parameters\n",
      "pdm_blocks\n",
      "pre_enc\n",
      "predict\n",
      "predict_dataloader\n",
      "predict_layers\n",
      "predict_step\n",
      "prepare_data\n",
      "prepare_data_per_node\n",
      "preprocess\n",
      "print\n",
      "projection_layer\n",
      "random_seed\n",
      "register_backward_hook\n",
      "register_buffer\n",
      "register_forward_hook\n",
      "register_forward_pre_hook\n",
      "register_full_backward_hook\n",
      "register_full_backward_pre_hook\n",
      "register_load_state_dict_post_hook\n",
      "register_module\n",
      "register_parameter\n",
      "register_state_dict_pre_hook\n",
      "regression_layers\n",
      "requires_grad_\n",
      "save\n",
      "save_hyperparameters\n",
      "scaler\n",
      "set_extra_state\n",
      "set_test_size\n",
      "setup\n",
      "share_memory\n",
      "stat_exog_list\n",
      "stat_exog_size\n",
      "state_dict\n",
      "step_size\n",
      "strict_loading\n",
      "teardown\n",
      "test_dataloader\n",
      "test_size\n",
      "test_step\n",
      "to\n",
      "to_empty\n",
      "to_onnx\n",
      "to_torchscript\n",
      "toggle_optimizer\n",
      "top_k\n",
      "train\n",
      "train_dataloader\n",
      "train_trajectories\n",
      "trainer\n",
      "trainer_kwargs\n",
      "training\n",
      "training_step\n",
      "transfer_batch_to_device\n",
      "type\n",
      "unfreeze\n",
      "untoggle_optimizer\n",
      "use_future_temporal_feature\n",
      "use_norm\n",
      "val_check_steps\n",
      "val_dataloader\n",
      "val_size\n",
      "valid_loss\n",
      "valid_trajectories\n",
      "validation_step\n",
      "validation_step_outputs\n",
      "xpu\n",
      "zero_grad\n"
     ]
    }
   ],
   "source": [
    "# list out all the attributes for `nf.models[0]`\n",
    "attributes = dir(nf.models[0])\n",
    "for attribute in attributes:\n",
    "    print(attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1.2652792930603027),\n",
       " (1, 1.4556605815887451),\n",
       " (2, 1.913306474685669),\n",
       " (3, 1.648797631263733),\n",
       " (4, 1.6551233530044556),\n",
       " (5, 1.3696376085281372),\n",
       " (6, 1.5856698751449585),\n",
       " (7, 1.5500593185424805),\n",
       " (8, 1.4477684497833252),\n",
       " (9, 1.6019489765167236),\n",
       " (10, 1.5306743383407593),\n",
       " (11, 1.6930204629898071),\n",
       " (12, 1.2258554697036743),\n",
       " (13, 1.426756739616394),\n",
       " (14, 1.477355718612671),\n",
       " (15, 1.4579471349716187),\n",
       " (16, 1.6078250408172607),\n",
       " (17, 1.4272948503494263),\n",
       " (18, 1.4014198780059814),\n",
       " (19, 1.6109946966171265),\n",
       " (20, 1.7012355327606201),\n",
       " (21, 1.4736907482147217),\n",
       " (22, 1.4918445348739624),\n",
       " (23, 1.5586013793945312),\n",
       " (24, 1.4386353492736816),\n",
       " (25, 1.4953359365463257),\n",
       " (26, 1.3780930042266846),\n",
       " (27, 1.5182558298110962),\n",
       " (28, 1.455914855003357),\n",
       " (29, 1.5370842218399048),\n",
       " (30, 1.5447022914886475),\n",
       " (31, 1.468395471572876),\n",
       " (32, 1.3750426769256592),\n",
       " (33, 1.3290830850601196),\n",
       " (34, 1.4182649850845337),\n",
       " (35, 1.4127308130264282),\n",
       " (36, 1.5263346433639526),\n",
       " (37, 1.6186168193817139),\n",
       " (38, 1.8248249292373657),\n",
       " (39, 1.767852544784546),\n",
       " (40, 1.3463252782821655),\n",
       " (41, 1.3858100175857544),\n",
       " (42, 1.709517240524292),\n",
       " (43, 1.774518370628357),\n",
       " (44, 1.5143324136734009),\n",
       " (45, 1.538897156715393),\n",
       " (46, 1.3223159313201904),\n",
       " (47, 1.61931574344635),\n",
       " (48, 1.551661729812622),\n",
       " (49, 1.4490735530853271),\n",
       " (50, 1.528573989868164),\n",
       " (51, 1.370074987411499),\n",
       " (52, 1.6495513916015625),\n",
       " (53, 1.3800313472747803),\n",
       " (54, 1.5439108610153198),\n",
       " (55, 1.638758897781372),\n",
       " (56, 1.4633312225341797),\n",
       " (57, 1.4911822080612183),\n",
       " (58, 1.6075584888458252),\n",
       " (59, 1.437645435333252),\n",
       " (60, 1.5779188871383667),\n",
       " (61, 1.463838815689087),\n",
       " (62, 1.4644778966903687),\n",
       " (63, 1.637426733970642),\n",
       " (64, 1.7975962162017822),\n",
       " (65, 1.4528840780258179),\n",
       " (66, 1.415134072303772),\n",
       " (67, 1.3850057125091553),\n",
       " (68, 1.4359601736068726),\n",
       " (69, 1.3556286096572876),\n",
       " (70, 1.5640842914581299),\n",
       " (71, 1.3321164846420288),\n",
       " (72, 1.5388057231903076),\n",
       " (73, 1.2425172328948975),\n",
       " (74, 1.6500968933105469),\n",
       " (75, 1.5435490608215332),\n",
       " (76, 1.408852458000183),\n",
       " (77, 1.4056302309036255),\n",
       " (78, 1.6050786972045898),\n",
       " (79, 1.474798321723938),\n",
       " (80, 1.2987706661224365),\n",
       " (81, 1.4466017484664917),\n",
       " (82, 1.6079130172729492),\n",
       " (83, 1.522503137588501),\n",
       " (84, 1.939475417137146),\n",
       " (85, 1.563033103942871),\n",
       " (86, 1.5458983182907104),\n",
       " (87, 1.409125566482544),\n",
       " (88, 1.498255729675293),\n",
       " (89, 1.3994700908660889),\n",
       " (90, 1.4071096181869507),\n",
       " (91, 1.5888843536376953),\n",
       " (92, 1.6894086599349976),\n",
       " (93, 1.6499364376068115),\n",
       " (94, 1.5229490995407104),\n",
       " (95, 1.5380151271820068),\n",
       " (96, 1.451241135597229),\n",
       " (97, 1.6189830303192139),\n",
       " (98, 1.723339557647705),\n",
       " (99, 1.5948312282562256),\n",
       " (100, 1.281877040863037),\n",
       " (101, 1.4097959995269775),\n",
       " (102, 1.5478508472442627),\n",
       " (103, 1.4514354467391968),\n",
       " (104, 1.2826449871063232),\n",
       " (105, 1.4328571557998657),\n",
       " (106, 1.436640739440918),\n",
       " (107, 1.5618071556091309),\n",
       " (108, 1.4744608402252197),\n",
       " (109, 1.5322239398956299),\n",
       " (110, 1.33432936668396),\n",
       " (111, 1.4284825325012207),\n",
       " (112, 1.525911569595337),\n",
       " (113, 1.631467580795288),\n",
       " (114, 1.6013238430023193),\n",
       " (115, 1.6004409790039062),\n",
       " (116, 1.4519941806793213),\n",
       " (117, 1.5018856525421143),\n",
       " (118, 1.4538688659667969),\n",
       " (119, 1.3175710439682007),\n",
       " (120, 1.6426286697387695),\n",
       " (121, 1.7258306741714478),\n",
       " (122, 1.599919080734253),\n",
       " (123, 1.4623205661773682),\n",
       " (124, 1.486581563949585),\n",
       " (125, 1.4983954429626465),\n",
       " (126, 1.4807472229003906),\n",
       " (127, 1.223907470703125),\n",
       " (128, 1.768527626991272),\n",
       " (129, 1.3526647090911865),\n",
       " (130, 1.7069165706634521),\n",
       " (131, 1.6436703205108643),\n",
       " (132, 1.6337871551513672),\n",
       " (133, 1.4297025203704834),\n",
       " (134, 1.4796805381774902),\n",
       " (135, 1.5518990755081177),\n",
       " (136, 1.3426189422607422),\n",
       " (137, 1.4552562236785889),\n",
       " (138, 1.3924331665039062),\n",
       " (139, 1.6326738595962524),\n",
       " (140, 1.548612356185913),\n",
       " (141, 1.5016047954559326),\n",
       " (142, 1.5074963569641113),\n",
       " (143, 1.5194603204727173),\n",
       " (144, 1.7502295970916748),\n",
       " (145, 1.5721577405929565),\n",
       " (146, 1.4064499139785767),\n",
       " (147, 1.4745203256607056),\n",
       " (148, 1.4154201745986938),\n",
       " (149, 1.645202398300171),\n",
       " (150, 1.3629778623580933),\n",
       " (151, 1.622849464416504),\n",
       " (152, 1.6603381633758545),\n",
       " (153, 1.636135458946228),\n",
       " (154, 1.7083696126937866),\n",
       " (155, 1.3740646839141846),\n",
       " (156, 1.4133507013320923),\n",
       " (157, 1.6005512475967407),\n",
       " (158, 1.712479829788208),\n",
       " (159, 1.4828990697860718),\n",
       " (160, 1.3030740022659302),\n",
       " (161, 1.6019490957260132),\n",
       " (162, 1.5483930110931396),\n",
       " (163, 1.4548835754394531),\n",
       " (164, 1.5339313745498657),\n",
       " (165, 1.5509021282196045),\n",
       " (166, 1.416431188583374),\n",
       " (167, 1.3164050579071045),\n",
       " (168, 1.32805597782135),\n",
       " (169, 1.6914188861846924),\n",
       " (170, 1.2719597816467285),\n",
       " (171, 1.529478669166565),\n",
       " (172, 1.72683846950531),\n",
       " (173, 1.4261579513549805),\n",
       " (174, 1.3379480838775635),\n",
       " (175, 1.5691261291503906),\n",
       " (176, 1.7987382411956787),\n",
       " (177, 1.4030535221099854),\n",
       " (178, 1.7061755657196045),\n",
       " (179, 1.4131314754486084),\n",
       " (180, 1.4183459281921387),\n",
       " (181, 1.44047212600708),\n",
       " (182, 1.4759137630462646),\n",
       " (183, 1.257108211517334),\n",
       " (184, 1.6143395900726318),\n",
       " (185, 1.4363051652908325),\n",
       " (186, 1.575758695602417),\n",
       " (187, 1.3471884727478027),\n",
       " (188, 1.6050983667373657),\n",
       " (189, 1.5756727457046509),\n",
       " (190, 1.5828123092651367),\n",
       " (191, 1.5773106813430786),\n",
       " (192, 1.3177238702774048),\n",
       " (193, 1.3793315887451172),\n",
       " (194, 1.5131725072860718),\n",
       " (195, 1.4755876064300537),\n",
       " (196, 1.5258185863494873),\n",
       " (197, 1.3437517881393433),\n",
       " (198, 1.3883247375488281),\n",
       " (199, 1.4992715120315552),\n",
       " (200, 1.3532181978225708),\n",
       " (201, 1.4853039979934692),\n",
       " (202, 1.5011022090911865),\n",
       " (203, 1.6132665872573853),\n",
       " (204, 1.4772299528121948),\n",
       " (205, 1.4696170091629028),\n",
       " (206, 1.407053828239441),\n",
       " (207, 1.615049958229065),\n",
       " (208, 1.7406184673309326),\n",
       " (209, 1.295318841934204),\n",
       " (210, 1.3820672035217285),\n",
       " (211, 1.3333425521850586),\n",
       " (212, 1.7166402339935303),\n",
       " (213, 1.6864509582519531),\n",
       " (214, 1.3871345520019531),\n",
       " (215, 1.378572702407837),\n",
       " (216, 1.575943946838379),\n",
       " (217, 1.445293664932251),\n",
       " (218, 1.449353575706482),\n",
       " (219, 1.402377963066101),\n",
       " (220, 1.5070381164550781),\n",
       " (221, 1.590567946434021),\n",
       " (222, 1.4441568851470947),\n",
       " (223, 1.287078619003296),\n",
       " (224, 1.4576259851455688),\n",
       " (225, 1.674619436264038),\n",
       " (226, 1.4202587604522705),\n",
       " (227, 1.697028398513794),\n",
       " (228, 1.3972642421722412),\n",
       " (229, 1.3520052433013916),\n",
       " (230, 1.3901015520095825),\n",
       " (231, 1.4575715065002441),\n",
       " (232, 1.4327330589294434),\n",
       " (233, 1.3254716396331787),\n",
       " (234, 1.553039789199829),\n",
       " (235, 1.3164708614349365),\n",
       " (236, 1.5127803087234497),\n",
       " (237, 1.526930570602417),\n",
       " (238, 1.475691795349121),\n",
       " (239, 1.4029473066329956),\n",
       " (240, 1.353986144065857),\n",
       " (241, 1.6892855167388916),\n",
       " (242, 1.279772162437439),\n",
       " (243, 1.512436032295227),\n",
       " (244, 1.4459813833236694),\n",
       " (245, 1.461891531944275),\n",
       " (246, 1.4074736833572388),\n",
       " (247, 1.487505555152893),\n",
       " (248, 1.473873257637024),\n",
       " (249, 1.5040013790130615),\n",
       " (250, 1.3844584226608276),\n",
       " (251, 1.420502781867981),\n",
       " (252, 1.3820888996124268),\n",
       " (253, 1.4289467334747314),\n",
       " (254, 1.732461929321289),\n",
       " (255, 1.5795634984970093),\n",
       " (256, 1.384515643119812),\n",
       " (257, 1.463347315788269),\n",
       " (258, 1.4286354780197144),\n",
       " (259, 1.3722983598709106),\n",
       " (260, 1.4536457061767578),\n",
       " (261, 1.3758362531661987),\n",
       " (262, 1.5039979219436646),\n",
       " (263, 1.349236249923706),\n",
       " (264, 1.523991346359253),\n",
       " (265, 1.256334900856018),\n",
       " (266, 1.5908821821212769),\n",
       " (267, 1.4560397863388062),\n",
       " (268, 1.37725830078125),\n",
       " (269, 1.5198345184326172),\n",
       " (270, 1.4770333766937256),\n",
       " (271, 1.5293593406677246),\n",
       " (272, 1.2407686710357666),\n",
       " (273, 1.4555877447128296),\n",
       " (274, 1.2600159645080566),\n",
       " (275, 1.5748131275177002),\n",
       " (276, 1.5905332565307617),\n",
       " (277, 1.4303890466690063),\n",
       " (278, 1.5766117572784424),\n",
       " (279, 1.4373136758804321),\n",
       " (280, 1.611547589302063),\n",
       " (281, 1.4219809770584106),\n",
       " (282, 1.495540738105774),\n",
       " (283, 1.5473207235336304),\n",
       " (284, 1.4510687589645386),\n",
       " (285, 1.5787694454193115),\n",
       " (286, 1.3853305578231812),\n",
       " (287, 1.6494709253311157),\n",
       " (288, 1.5801026821136475),\n",
       " (289, 1.61037278175354),\n",
       " (290, 1.3217670917510986),\n",
       " (291, 1.2933075428009033),\n",
       " (292, 1.4322230815887451),\n",
       " (293, 1.3795883655548096),\n",
       " (294, 1.3525569438934326),\n",
       " (295, 1.2557815313339233),\n",
       " (296, 1.3802231550216675),\n",
       " (297, 1.4779253005981445),\n",
       " (298, 1.6269779205322266),\n",
       " (299, 1.3155311346054077),\n",
       " (300, 1.4150495529174805),\n",
       " (301, 1.4588408470153809),\n",
       " (302, 1.3405158519744873),\n",
       " (303, 1.4158599376678467),\n",
       " (304, 1.4680935144424438),\n",
       " (305, 1.207073450088501),\n",
       " (306, 1.4746488332748413),\n",
       " (307, 1.5188581943511963),\n",
       " (308, 1.3712158203125),\n",
       " (309, 1.3785698413848877),\n",
       " (310, 1.4682948589324951),\n",
       " (311, 1.590323805809021),\n",
       " (312, 1.4167238473892212),\n",
       " (313, 1.6480343341827393),\n",
       " (314, 1.3428095579147339),\n",
       " (315, 1.3668919801712036),\n",
       " (316, 1.443681001663208),\n",
       " (317, 1.313848614692688),\n",
       " (318, 1.3262286186218262),\n",
       " (319, 1.553343415260315),\n",
       " (320, 1.4789636135101318),\n",
       " (321, 1.5362396240234375),\n",
       " (322, 1.3496733903884888),\n",
       " (323, 1.3188508749008179),\n",
       " (324, 1.540717601776123),\n",
       " (325, 1.4736251831054688),\n",
       " (326, 1.3596891164779663),\n",
       " (327, 1.2310292720794678),\n",
       " (328, 1.8840243816375732),\n",
       " (329, 1.4598348140716553),\n",
       " (330, 1.4714393615722656),\n",
       " (331, 1.300218939781189),\n",
       " (332, 1.5013914108276367),\n",
       " (333, 1.5904951095581055),\n",
       " (334, 1.6038010120391846),\n",
       " (335, 1.4868781566619873),\n",
       " (336, 1.4274766445159912),\n",
       " (337, 1.40560781955719),\n",
       " (338, 1.4119850397109985),\n",
       " (339, 1.651784896850586),\n",
       " (340, 1.2836929559707642),\n",
       " (341, 1.4594523906707764),\n",
       " (342, 1.5395103693008423),\n",
       " (343, 1.3894386291503906),\n",
       " (344, 1.6368297338485718),\n",
       " (345, 1.4431257247924805),\n",
       " (346, 1.5883612632751465),\n",
       " (347, 1.6111743450164795),\n",
       " (348, 1.5108236074447632),\n",
       " (349, 1.4336503744125366),\n",
       " (350, 1.4791007041931152),\n",
       " (351, 1.5484983921051025),\n",
       " (352, 1.380267858505249),\n",
       " (353, 1.4854519367218018),\n",
       " (354, 1.426681399345398),\n",
       " (355, 1.4555212259292603),\n",
       " (356, 1.3132727146148682),\n",
       " (357, 1.3070887327194214),\n",
       " (358, 1.451190710067749),\n",
       " (359, 1.2165043354034424),\n",
       " (360, 1.3072540760040283),\n",
       " (361, 1.3294076919555664),\n",
       " (362, 1.3546346426010132),\n",
       " (363, 1.295459270477295),\n",
       " (364, 1.5878151655197144),\n",
       " (365, 1.4138116836547852),\n",
       " (366, 1.3622355461120605),\n",
       " (367, 1.3287429809570312),\n",
       " (368, 1.562183141708374),\n",
       " (369, 1.528537631034851),\n",
       " (370, 1.6120624542236328),\n",
       " (371, 1.4473851919174194),\n",
       " (372, 1.4076247215270996),\n",
       " (373, 1.3193424940109253),\n",
       " (374, 1.3387237787246704),\n",
       " (375, 1.4032719135284424),\n",
       " (376, 1.4516624212265015),\n",
       " (377, 1.645981788635254),\n",
       " (378, 1.4324010610580444),\n",
       " (379, 1.5180132389068604),\n",
       " (380, 1.4232655763626099),\n",
       " (381, 1.4320992231369019),\n",
       " (382, 1.4398859739303589),\n",
       " (383, 1.629699945449829),\n",
       " (384, 1.5197583436965942),\n",
       " (385, 1.4628907442092896),\n",
       " (386, 1.4840346574783325),\n",
       " (387, 1.4818533658981323),\n",
       " (388, 1.450669527053833),\n",
       " (389, 1.4464080333709717),\n",
       " (390, 1.4524972438812256),\n",
       " (391, 1.3642041683197021),\n",
       " (392, 1.4810060262680054),\n",
       " (393, 1.347908616065979),\n",
       " (394, 1.556548833847046),\n",
       " (395, 1.6606470346450806),\n",
       " (396, 1.3990792036056519),\n",
       " (397, 1.4025166034698486),\n",
       " (398, 1.217109203338623),\n",
       " (399, 1.2416012287139893),\n",
       " (400, 1.472447156906128),\n",
       " (401, 1.4375160932540894),\n",
       " (402, 1.3948349952697754),\n",
       " (403, 1.4906642436981201),\n",
       " (404, 1.5209070444107056),\n",
       " (405, 1.4691967964172363),\n",
       " (406, 1.5185941457748413),\n",
       " (407, 1.379479169845581),\n",
       " (408, 1.4126217365264893),\n",
       " (409, 1.388776183128357),\n",
       " (410, 1.4891357421875),\n",
       " (411, 1.546705961227417),\n",
       " (412, 1.4946914911270142),\n",
       " (413, 1.6088380813598633),\n",
       " (414, 1.4977556467056274),\n",
       " (415, 1.5012110471725464),\n",
       " (416, 1.4231081008911133),\n",
       " (417, 1.5674591064453125),\n",
       " (418, 1.4991470575332642),\n",
       " (419, 1.2922252416610718),\n",
       " (420, 1.3098281621932983),\n",
       " (421, 1.411011815071106),\n",
       " (422, 1.4608738422393799),\n",
       " (423, 1.5043400526046753),\n",
       " (424, 1.4712778329849243),\n",
       " (425, 1.4305897951126099),\n",
       " (426, 1.4338016510009766),\n",
       " (427, 1.5056411027908325),\n",
       " (428, 1.5089797973632812),\n",
       " (429, 1.4064135551452637),\n",
       " (430, 1.5492790937423706),\n",
       " (431, 1.294633150100708),\n",
       " (432, 1.4060522317886353),\n",
       " (433, 1.447259783744812),\n",
       " (434, 1.3844801187515259),\n",
       " (435, 1.4699097871780396),\n",
       " (436, 1.5232069492340088),\n",
       " (437, 1.4941937923431396),\n",
       " (438, 1.2238869667053223),\n",
       " (439, 1.2409284114837646),\n",
       " (440, 1.3805367946624756),\n",
       " (441, 1.619328498840332),\n",
       " (442, 1.424560785293579),\n",
       " (443, 1.3601067066192627),\n",
       " (444, 1.4873666763305664),\n",
       " (445, 1.3363111019134521),\n",
       " (446, 1.6375755071640015),\n",
       " (447, 1.4219322204589844),\n",
       " (448, 1.6101303100585938),\n",
       " (449, 1.4108281135559082),\n",
       " (450, 1.4993746280670166),\n",
       " (451, 1.267255425453186),\n",
       " (452, 1.6397554874420166),\n",
       " (453, 1.3687387704849243),\n",
       " (454, 1.5374360084533691),\n",
       " (455, 1.4175697565078735),\n",
       " (456, 1.2645434141159058),\n",
       " (457, 1.579211950302124),\n",
       " (458, 1.3692972660064697),\n",
       " (459, 1.2739912271499634),\n",
       " (460, 1.5289658308029175),\n",
       " (461, 1.4358417987823486),\n",
       " (462, 1.4368746280670166),\n",
       " (463, 1.4812462329864502),\n",
       " (464, 1.394964575767517),\n",
       " (465, 1.4111101627349854),\n",
       " (466, 1.3339803218841553),\n",
       " (467, 1.3361872434616089),\n",
       " (468, 1.3773599863052368),\n",
       " (469, 1.4215625524520874),\n",
       " (470, 1.3958758115768433),\n",
       " (471, 1.4118502140045166),\n",
       " (472, 1.3864322900772095),\n",
       " (473, 1.4722331762313843),\n",
       " (474, 1.7676079273223877),\n",
       " (475, 1.3411840200424194),\n",
       " (476, 1.3572571277618408),\n",
       " (477, 1.3651940822601318),\n",
       " (478, 1.3520870208740234),\n",
       " (479, 1.322898030281067),\n",
       " (480, 1.256388545036316),\n",
       " (481, 1.4366446733474731),\n",
       " (482, 1.3747191429138184),\n",
       " (483, 1.315680742263794),\n",
       " (484, 1.414306402206421),\n",
       " (485, 1.6069344282150269),\n",
       " (486, 1.438069462776184),\n",
       " (487, 1.3182260990142822),\n",
       " (488, 1.4743866920471191),\n",
       " (489, 1.243495225906372),\n",
       " (490, 1.2059284448623657),\n",
       " (491, 1.3767902851104736),\n",
       " (492, 1.3974754810333252),\n",
       " (493, 1.399251937866211),\n",
       " (494, 1.4710534811019897),\n",
       " (495, 1.4349952936172485),\n",
       " (496, 1.4823808670043945),\n",
       " (497, 1.366346001625061),\n",
       " (498, 1.3001213073730469),\n",
       " (499, 1.1497981548309326),\n",
       " (500, 1.4484196901321411),\n",
       " (501, 1.4697437286376953),\n",
       " (502, 1.4684017896652222),\n",
       " (503, 1.404890537261963),\n",
       " (504, 1.3779704570770264),\n",
       " (505, 1.3441888093948364),\n",
       " (506, 1.343747854232788),\n",
       " (507, 1.3975231647491455),\n",
       " (508, 1.3545782566070557),\n",
       " (509, 1.45951247215271),\n",
       " (510, 1.4707653522491455),\n",
       " (511, 1.227542757987976),\n",
       " (512, 1.4056320190429688),\n",
       " (513, 1.282214879989624),\n",
       " (514, 1.4265488386154175),\n",
       " (515, 1.269141435623169),\n",
       " (516, 1.5319907665252686),\n",
       " (517, 1.3371907472610474),\n",
       " (518, 1.3420181274414062),\n",
       " (519, 1.2797353267669678),\n",
       " (520, 1.4880380630493164),\n",
       " (521, 1.311663031578064),\n",
       " (522, 1.3042223453521729),\n",
       " (523, 1.5270602703094482),\n",
       " (524, 1.26394522190094),\n",
       " (525, 1.5589287281036377),\n",
       " (526, 1.3283225297927856),\n",
       " (527, 1.4053988456726074),\n",
       " (528, 1.215920329093933),\n",
       " (529, 1.2686055898666382),\n",
       " (530, 1.2348055839538574),\n",
       " (531, 1.4364893436431885),\n",
       " (532, 1.3619520664215088),\n",
       " (533, 1.4713335037231445),\n",
       " (534, 1.4284565448760986),\n",
       " (535, 1.395641803741455),\n",
       " (536, 1.389932632446289),\n",
       " (537, 1.2699286937713623),\n",
       " (538, 1.1508084535598755),\n",
       " (539, 1.2688324451446533),\n",
       " (540, 1.290855050086975),\n",
       " (541, 1.4365437030792236),\n",
       " (542, 1.5447132587432861),\n",
       " (543, 1.6796844005584717),\n",
       " (544, 1.4097368717193604),\n",
       " (545, 1.3895385265350342),\n",
       " (546, 1.450178623199463),\n",
       " (547, 1.2791005373001099),\n",
       " (548, 1.2881110906600952),\n",
       " (549, 1.314574956893921),\n",
       " (550, 1.1313724517822266),\n",
       " (551, 1.2091351747512817),\n",
       " (552, 1.1830765008926392),\n",
       " (553, 1.250671625137329),\n",
       " (554, 1.3191171884536743),\n",
       " (555, 1.2746974229812622),\n",
       " (556, 1.2287901639938354),\n",
       " (557, 1.411582589149475),\n",
       " (558, 1.3946994543075562),\n",
       " (559, 1.2663793563842773),\n",
       " (560, 1.3366786241531372),\n",
       " (561, 1.4114909172058105),\n",
       " (562, 1.2808492183685303),\n",
       " (563, 1.5195889472961426),\n",
       " (564, 1.4710420370101929),\n",
       " (565, 1.3810604810714722),\n",
       " (566, 1.262549638748169),\n",
       " (567, 1.2831768989562988),\n",
       " (568, 1.2924463748931885),\n",
       " (569, 1.280245065689087),\n",
       " (570, 1.313549280166626),\n",
       " (571, 1.3913261890411377),\n",
       " (572, 1.3490928411483765),\n",
       " (573, 1.353149652481079),\n",
       " (574, 1.442859411239624),\n",
       " (575, 1.3780884742736816),\n",
       " (576, 1.1915154457092285),\n",
       " (577, 1.2779438495635986),\n",
       " (578, 1.2196416854858398),\n",
       " (579, 1.4947826862335205),\n",
       " (580, 1.3612759113311768),\n",
       " (581, 1.2349563837051392),\n",
       " (582, 1.251711130142212),\n",
       " (583, 1.2902953624725342),\n",
       " (584, 1.343078374862671),\n",
       " (585, 1.2971402406692505),\n",
       " (586, 1.2145729064941406),\n",
       " (587, 1.141843557357788),\n",
       " (588, 1.4761422872543335),\n",
       " (589, 1.4178338050842285),\n",
       " (590, 1.4227006435394287),\n",
       " (591, 1.3990728855133057),\n",
       " (592, 1.4240158796310425),\n",
       " (593, 1.3612054586410522),\n",
       " (594, 1.3466925621032715),\n",
       " (595, 1.3238543272018433),\n",
       " (596, 1.3115103244781494),\n",
       " (597, 1.3019993305206299),\n",
       " (598, 1.34761643409729),\n",
       " (599, 1.386099934577942)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_losses = nf.models[0].train_trajectories\n",
    "train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1.2441478967666626),\n",
       " (100, 1.2389733791351318),\n",
       " (200, 1.2499414682388306),\n",
       " (300, 1.2595993280410767),\n",
       " (400, 1.2697396278381348),\n",
       " (500, 1.314969778060913),\n",
       " (600, 1.3116085529327393)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_losses = nf.models[0].valid_trajectories\n",
    "valid_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1313724517822266"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the smallest loss from `train_losses`, which is a list of 2-element tuples, and the 2nd element is loss value.\n",
    "smallest_loss = min(train_losses, key=lambda x: x[1])[1]\n",
    "smallest_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2389733791351318"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(valid_losses, key=lambda x: x[1])[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Troubleshoot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RuntimeError('The size of tensor a (3) must match the size of tensor b (4) at non-singleton dimension 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams_raw = {\n",
    "    \"h\": 20,\n",
    "    \"d_ff\": 32,\n",
    "    \"top_k\": 9,\n",
    "    \"d_model\": 32,\n",
    "    \"dropout\": 0.025678663612217545,\n",
    "    \"e_layers\": 15,\n",
    "    \"use_norm\": False,\n",
    "    \"optimizer\": \"SGD\",\n",
    "    \"batch_size\": 64,\n",
    "    \"covar_dist\": [\n",
    "        0.012446164040523651,\n",
    "        0.07434833727501454,\n",
    "        0.005954535986635773,\n",
    "        0.0459394531992627,\n",
    "        0.1216363214684387,\n",
    "        0.31218214537049555,\n",
    "        0.008661552616809226,\n",
    "        0.0914576620330267,\n",
    "        0.10292342609136783,\n",
    "        0.01150455517290314,\n",
    "        0.006043906561017433,\n",
    "        0.04094889296331747,\n",
    "        0.08992125574660075,\n",
    "        0.07603179147458679,\n",
    "    ],\n",
    "    \"input_size\": 678,\n",
    "    \"moving_avg\": 25,\n",
    "    \"topk_covar\": 7,\n",
    "    \"decomp_method\": \"moving_avg\",\n",
    "    \"learning_rate\": 0.00020395846998135436,\n",
    "    \"local_scaler_type\": \"robust-iqr\",\n",
    "    \"channel_independence\": 1,\n",
    "    \"down_sampling_layers\": 2,\n",
    "    \"down_sampling_method\": \"conv\",\n",
    "    \"down_sampling_window\": 13,\n",
    "    \"early_stop_patience_steps\": 15,\n",
    "    \"accelerator\": \"cpu\",\n",
    "    \"devices\": 1,\n",
    "    \"max_steps\": 1000,\n",
    "    \"val_check_steps\": 50,\n",
    "    \"validate\": True,\n",
    "}\n",
    "# hparams_raw = {\n",
    "#     \"batch_size\": 32,\n",
    "#     \"channel_independence\": 0,\n",
    "#     \"covar_dist\": [\n",
    "#         0.05941820259601838,\n",
    "#         0.004731065485515826,\n",
    "#         0.14343522095659036,\n",
    "#         0.09697409120321702,\n",
    "#         0.10287015087717696,\n",
    "#         0.06997545077365976,\n",
    "#         0.022408840619061114,\n",
    "#         0.018386297819442635,\n",
    "#         0.0664151536605972,\n",
    "#         0.02222760702564122,\n",
    "#         0.07518915528342811,\n",
    "#         0.0804237066160445,\n",
    "#         0.04750083522628452,\n",
    "#         0.19004422185732242,\n",
    "#     ],\n",
    "#     \"d_ff\": 128,\n",
    "#     \"d_model\": 256,\n",
    "#     \"decomp_method\": \"moving_avg\",\n",
    "#     \"down_sampling_layers\": 7,\n",
    "#     \"down_sampling_method\": \"max\",\n",
    "#     \"down_sampling_window\": 4,\n",
    "#     \"dropout\": 0.33971283243512906,\n",
    "#     \"e_layers\": 15,\n",
    "#     \"early_stop_patience_steps\": 11,\n",
    "#     \"input_size\": 13,\n",
    "#     \"learning_rate\": 0.001032553544411743,\n",
    "#     \"local_scaler_type\": \"standard\",\n",
    "#     \"moving_avg\": 33,\n",
    "#     \"optimizer\": \"Adam\",\n",
    "#     \"top_k\": 6,\n",
    "#     \"use_norm\": True,\n",
    "#     \"h\": 20,\n",
    "#     \"max_steps\": 1000,\n",
    "#     \"validate\": True,\n",
    "#     \"random_seed\": 7,\n",
    "#     \"accelerator\": \"cpu\",\n",
    "#     \"devices\": 1,\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove \"validate\", \"covar_dist\" from hparams_raw dict\n",
    "validate = hparams_raw.pop('validate', None)\n",
    "hparams_raw.pop('covar_dist', None)\n",
    "hparams_raw.pop(\"topk_covar\", None)\n",
    "local_scaler_type = hparams_raw.pop(\"local_scaler_type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim.optimizer import Optimizer\n",
    "\n",
    "def _select_optimizer(**kwargs):\n",
    "    match kwargs[\"optimizer\"]:\n",
    "        case \"Adam\":\n",
    "            model_optim = optim.Adam\n",
    "        case \"AdamW\":\n",
    "            model_optim = optim.AdamW\n",
    "        case \"SGD\":\n",
    "            model_optim = optim.SGD\n",
    "    optim_args = {\n",
    "        \"lr\": kwargs[\"learning_rate\"],\n",
    "    }\n",
    "    if kwargs[\"optimizer\"] != \"SGD\":\n",
    "        optim_args[\"fused\"] = (\n",
    "            kwargs[\"accelerator\"] in (\"gpu\", \"auto\") and torch.cuda.is_available()\n",
    "        )\n",
    "    return model_optim, optim_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SGD'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer, optim_args = _select_optimizer(**hparams_raw)\n",
    "hparams_raw.pop(\"optimizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_logger = logging.getLogger(\"lightning_fabric.utilities.seed\")\n",
    "orig_seed_log_level = seed_logger.getEffectiveLevel()\n",
    "seed_logger.setLevel(logging.FATAL)\n",
    "\n",
    "tmx = TimeMixer(\n",
    "    n_series=1,\n",
    "    loss=HuberLoss(),\n",
    "    enable_progress_bar=False,\n",
    "    enable_model_summary=False,\n",
    "    optimizer=optimizer,\n",
    "    optimizer_kwargs=optim_args,\n",
    "    **hparams_raw,\n",
    ")\n",
    "\n",
    "nf = NeuralForecast(\n",
    "    models=[tmx],\n",
    "    freq=\"B\",\n",
    "    local_scaler_type=local_scaler_type,\n",
    ")\n",
    "\n",
    "seed_logger.setLevel(orig_seed_log_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jx/ProgramData/git/neuralforecast/neuralforecast/common/_base_model.py:379: UserWarning: ignoring learning rate passed in optimizer_kwargs, using the model's learning rate\n",
      "  warnings.warn(\n",
      "/Users/jx/.pyenv/versions/3.12.2/envs/venv_3.12.2/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "# validate = True\n",
    "val_size = min(300, int(len(df_train) * 0.9)) if validate else 0\n",
    "nf.fit(df_train, val_size=val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "math.ceil(0.00000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 57, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "\n",
    "class MovingAvg(nn.Module):\n",
    "    \"\"\"\n",
    "    Moving average block to highlight the trend of time series\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, kernel_size, stride):\n",
    "        super(MovingAvg, self).__init__()\n",
    "        if kernel_size <= 1:\n",
    "            raise ValueError(\"kernel_size must be greater than 1\")\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # padding on the both ends of time series\n",
    "        padding = (self.kernel_size - 1) // 2\n",
    "        front = x[:, 0:1, :].repeat(1, padding, 1)\n",
    "        end = x[:, -1:, :].repeat(1, padding + (self.kernel_size % 2 == 0), 1)\n",
    "        x = torch.cat([front, x, end], dim=1)\n",
    "        x = self.avg(x.permute(0, 2, 1))\n",
    "        x = x.permute(0, 2, 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Example usage\n",
    "x = torch.randn(1, 57, 10)  # Example input tensor\n",
    "kernel_size = 20\n",
    "stride = 1\n",
    "model = MovingAvg(kernel_size, stride)\n",
    "output = model(x)\n",
    "print(output.shape)  # Should print torch.Size([1, 56, 10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_3.12.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
